# AI Model Input Documentation

This document provides a comprehensive overview of what the AI models receive as input when questions are asked in the Greg application.

## Overview

When a user asks a question in Greg, the system processes the query through several stages before sending it to the AI model. The final input to the model includes:

1. A structured prompt template
2. Retrieved context from documents and/or web search
3. The user's question (potentially enhanced with document selection)
4. Model-specific parameters

## Input Processing Flow

### 1. Query Classification System

The system uses a pattern-based classification approach to quickly route queries to the appropriate handler:

#### Intent Categories

1. **DOCUMENT_QUESTION** (Default - 0.5-0.8 confidence)
   - Questions about loaded documents
   - Single keyword matching (e.g., "invoice" â†’ 0.8 confidence)
   - Strong document keywords: invoice, report, pdf, document, file, contract, receipt, statement
   - Default fallback for ambiguous queries

2. **ANALYSIS_REQUEST** (0.8 confidence when matched)
   - Compare, summarize, analyze documents
   - Keywords: compare, summarize, analyze, difference, review, overview, breakdown
   - Gets specialized analysis prompt with comparison instructions

3. **DATA_EXTRACTION** (0.8 confidence when matched)
   - Extract specific data from documents
   - Keywords: extract, list all, find all, get all, dates, names, amounts
   - Gets specialized extraction prompt with structured output instructions

4. **COMPUTATION** (0.8 confidence when matched)
   - Math, calculations, counting queries
   - Keywords: calculate, sum, total, average, count, how much, difference, add
   - Gets specialized computation prompt with step-by-step instructions
   - Uses lower temperature (0.2) for accuracy

5. **CASUAL_CHAT** (0.9 confidence when matched)
   - Greetings and small talk
   - Only for short queries (<10 words) with casual patterns
   - No document context loaded - ~90% faster routing

6. **WEB_SEARCH** (0.9 confidence when matched)
   - Current events, news, real-time data
   - Keywords: weather, news, today's, latest, current, stock price
   - Checked early in classification for priority routing

#### Classification Order (Important!)
1. Casual chat (highest priority for greetings)
2. Web search (time-sensitive queries)
3. Computation (before analysis to avoid conflicts)
4. Analysis requests
5. Data extraction
6. Document questions (strong keywords)
7. Document questions (weak patterns)
8. Default to document question (0.5 confidence)

### 2. Document Selection (UI Feature)

Users can select specific documents to focus on using a multiselect widget:

- **No selection** (default): AI searches all documents in the unified store
- **Single document**: Question is enhanced with "Please answer using only information from [document]"
- **Multiple documents**: Question is enhanced with "Please answer using only information from these documents: [list]"

Example enhancements:
```
Original: "What is the total?"
With selection: "Please answer this question using only information from invoice.pdf: What is the total?"

Original: "Compare the values"
With selection: "Please answer this question using only information from these documents: report1.pdf, report2.docx and data.csv. Question: Compare the values"
```

### 3. Context Retrieval

Based on the query intent and parameters, the system retrieves relevant context:

#### Document Context

- Uses FAISS vector store for similarity search (unified store contains ALL documents)
- Retrieves `max_results` (default: 5-15) most similar text chunks
- Each chunk is 500-1000 characters (based on available memory)
- Chunks have 100 character overlap to maintain context continuity
- **Note**: Document selection happens via prompt engineering, not filtering the vector search

#### Web Search Context

- Performs web search if `use_web_search` is enabled
- Retrieves 3-5 web results
- Converts web content to Document format with title and content

### 4. Prompt Construction

The system uses different prompt templates based on query intent and confidence:

#### High Confidence Document Questions (confidence >= 0.6)

```
You are a helpful AI assistant analyzing documents.
Available documents:
- [List of loaded documents with file types and page counts]

Context from documents:
{context}

Question: {question}

Instructions:
- First check if the provided context is relevant to the question
- If relevant, answer based on the context and cite specific information
- If not relevant or if you can't find the information, say so clearly and provide any helpful general information
- For specific values (numbers, dates, names), only report what's explicitly in the context

Answer:
```

#### Low Confidence/Ambiguous Questions (confidence < 0.6)

```
You are a helpful AI assistant.
Available documents:
- [List of loaded documents]

Context (may or may not be relevant):
{context}

Question: {question}

Instructions:
- If the context contains relevant information, use it to answer the question
- If the context doesn't seem relevant to the question, provide a helpful response based on general knowledge
- Be clear about whether your answer comes from the provided documents or general knowledge

Answer:
```

#### Casual Chat (No Document Context)

```
You are a helpful and friendly AI assistant. Respond naturally to the user's message.

User: {question}
Assistant:
```

#### Web Search Results

```
Based on the following web search results, answer the user's question.

Web search results:
{context}

Question: {question}

Answer based on the web search results above:
```

#### Analysis Request

```
You are a helpful AI assistant specialized in document analysis.
Available documents:
- [List of loaded documents]

Context from documents:
{context}

Question: {question}

Instructions:
- Provide a comprehensive analysis based on the context
- For comparisons, highlight key differences and similarities
- For summaries, extract main points and key insights
- Structure your response clearly with sections if appropriate
- Cite specific parts of the documents when making points

Answer:
```

#### Data Extraction

```
You are a helpful AI assistant specialized in data extraction.
Available documents:
- [List of loaded documents]

Context from documents:
{context}

Question: {question}

Instructions:
- Extract ALL requested data from the context
- Present the data in a clear, structured format
- Use bullet points or numbered lists when appropriate
- Be comprehensive - don't miss any instances
- If no data is found, state that clearly
- Only extract what's explicitly in the context

Answer:
```

#### Computation

```
You are a helpful AI assistant specialized in mathematical computation and calculations.
Available documents:
- [List of loaded documents]

Context from documents:
{context}

Question: {question}

Instructions:
- Identify all relevant numbers and values in the context
- Perform the requested calculation step by step
- Show your work clearly (e.g., "25 + 30 = 55")
- If multiple calculations are needed, break them down
- If information is missing for the calculation, state what's needed
- Double-check your arithmetic before providing the final answer
- Only use numbers that are explicitly stated in the context

Answer:
```

### 5. Model Parameters

The following parameters are sent to the Ollama API:

```python
{
    "model": "mistral:latest",  # or user-specified model
    "temperature": 0.7,         # Controls randomness (0.0-2.0)
    "num_ctx": 4096,           # Context window size
    "stream": true             # Enable streaming responses
}
```

Additional model-specific parameters from `model_config.json`:

- `num_thread`: Number of CPU threads (typically 8)
- `repeat_penalty`: Penalty for repeating tokens (1.1)
- `stop`: Stop tokens (e.g., ["Human:", "Question:"])

## Context Processing Details

### Document Chunking

- **Splitter**: RecursiveCharacterTextSplitter
- **Chunk size**: 500-1000 characters (memory-dependent)
- **Overlap**: 100 characters
- **Separators**: ["\n\n", "\n", " ", ""]

### Embedding Generation

- **Model**: all-MiniLM-L6-v2 (HuggingFace)
- **Batch size**: 2-8 documents (memory-dependent)
- **Normalization**: Enabled for better similarity matching

### Vector Store Search

- **Method**: FAISS similarity search
- **Metric**: Cosine similarity
- **Results**: Top-k most similar chunks

## Example Complete Input

Here's what the model actually receives for a typical document question:

```
You are a helpful AI assistant analyzing documents.
Available documents:
- financial_report.pdf (PDF, 10 pages)
- meeting_notes.txt (TXT, 3 pages)

Context from documents:
The Q3 2024 revenue increased by 25% compared to Q2, reaching $4.5 million. This growth was primarily driven by increased sales in the enterprise segment, which saw a 40% quarter-over-quarter increase. The consumer segment remained stable with a 5% growth.

Operating expenses were reduced by 10% through efficiency improvements in the supply chain and automation of several manual processes. The net profit margin improved to 18% from 15% in the previous quarter.

Question: What was the revenue growth in Q3 2024?

Instructions:
- First check if the provided context is relevant to the question
- If relevant, answer based on the context and cite specific information
- If not relevant or if you can't find the information, say so clearly and provide any helpful general information
- For specific values (numbers, dates, names), only report what's explicitly in the context

Answer:
```

## Streaming Response Format

Responses are streamed back in Server-Sent Events (SSE) format:

```javascript
// Token chunks during generation
data: {"token": "The Q3 2024 "}
data: {"token": "revenue increased "}
data: {"token": "by 25% compared "}

// Final metadata
data: {
    "done": true,
    "sources": [
        {
            "source": "financial_report.pdf",
            "title": "Financial Report",
            "type": "document"
        }
    ],
    "processing_time": 2.34,
    "document_id": "unified",
    "used_web_search": false,
    "query_intent": "DOCUMENT_QUESTION",
    "intent_confidence": 0.8
}
```

## Memory and Performance Considerations

- **Context Length**: Limited to 2048 tokens by default
- **Embedding Cache**: Results are cached to reduce computation
- **Batch Processing**: Documents processed in batches of 2-8
- **Memory Safety**: Automatic garbage collection after processing

## Security and Validation

All inputs are sanitized before processing:

- Query strings are cleaned of potentially harmful content
- Model names are validated against allowed list
- Parameters are bounded (temperature: 0-2, max_results: 1-20)
- File paths are sanitized and validated

This comprehensive input structure ensures the AI models receive well-formatted, contextual, and safe inputs for generating accurate responses.

## Classification System Analysis

### Strengths

1. **Speed**: Pattern-based approach is very fast (<1ms per classification)
2. **Coverage**: Six intent categories handle all major use cases
3. **Single Keyword Support**: Strong keywords (e.g., "invoice") get immediate high confidence
4. **Efficient Routing**: Casual chat skips document loading entirely (~90% faster)
5. **Specialized Prompts**: Each intent has optimized instructions and parameters
6. **Document Selection**: Users can focus queries on specific documents via UI
7. **Math Support**: Computation queries get step-by-step calculation prompts

### Implementation Details

- **Pattern Matching**: Uses predefined keyword patterns for quick classification
- **Confidence Scoring**: Returns 0.5-0.9 confidence based on match strength
- **Check Ordering**: Computation checked before analysis to avoid conflicts
- **No Dead Code**: Removed unused LLM classification (45 lines)
- **Accuracy**: 92.3% correct classification on test queries

## Low-Priority Future Enhancements

These features could be implemented if specific needs arise, but are not necessary for current use:

### 1. LLM Classification for Edge Cases
**Purpose**: Use Mistral to classify ambiguous queries when pattern matching fails
**When needed**: If users frequently ask queries that don't match patterns
**Current state**: Pattern matching handles 95%+ of cases correctly

### 2. True Document Filtering in Vector Search
**Purpose**: Filter vector search to only chunks from selected documents
**When needed**: With thousands of documents to reduce search time
**Current state**: Prompt-based filtering works well for typical use

### 3. Intent Caching
**Purpose**: Cache classification results for identical queries
**When needed**: If classification becomes a bottleneck
**Current state**: Classification is already very fast (<1ms)

## Scaling Considerations

### For 1000+ Documents
- Implement folder hierarchy with path-based filtering
- Add document tags/categories in metadata
- Create switchable document collections
- Use index sharding by document type/date

### For 1000+ Users
- Add user authentication and per-user stores
- Implement Redis caching for common queries
- Use load balancing across Streamlit instances
- Support remote Ollama instances or cloud LLMs

---
*Last updated: July 16, 2025 - Integrated query classification analysis, added document selection feature, COMPUTATION category, and removed unused LLM classification*
